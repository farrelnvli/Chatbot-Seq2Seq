!pip install tensorflow==2.5.0
import tensorflow as tf
print(tf.__version__)

import numpy as np 
import pandas as pd 
import tensorflow as tf
import os
import re
import tensorflow.keras
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K


class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        assert isinstance(input_shape, list)
        # membuat trainable weight
        self.W_a = self.add_weight(name='W_a',
                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.U_a = self.add_weight(name='U_a',
                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),
                                   initializer='uniform',
                                   trainable=True)
        self.V_a = self.add_weight(name='V_a',
                                   shape=tf.TensorShape((input_shape[0][2], 1)),
                                   initializer='uniform',
                                   trainable=True)

        super(AttentionLayer, self).build(input_shape)  

    def call(self, inputs, verbose=False):
        assert type(inputs) == list
        encoder_out_seq, decoder_out_seq = inputs
        if verbose:
            print('encoder_out_seq>', encoder_out_seq.shape)
            print('decoder_out_seq>', decoder_out_seq.shape)

        def energy_step(inputs, states):
            assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
            assert isinstance(states, list) or isinstance(states, tuple), assert_msg

            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]
            de_hidden = inputs.shape[-1]

            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)

            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1) 
            if verbose:
                print('Ua.h>', U_a_dot_h.shape)

            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)
            if verbose:
                print('Ws+Uh>', Ws_plus_Uh.shape)


            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)
            e_i = K.softmax(e_i)
 
            if verbose:
                print('ei>', e_i.shape)

            return e_i, [e_i]

        def context_step(inputs, states):
            assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
            assert isinstance(states, list) or isinstance(states, tuple), assert_msg

            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)
            if verbose:
                print('ci>', c_i.shape)
            return c_i, [c_i]

        fake_state_c = K.sum(encoder_out_seq, axis=1)
        fake_state_e = K.sum(encoder_out_seq, axis=2)  

        last_out, e_outputs, _ = K.rnn(
            energy_step, decoder_out_seq, [fake_state_e],
        )

        last_out, c_outputs, _ = K.rnn(
            context_step, e_outputs, [fake_state_c],
        )

        return c_outputs, e_outputs

    def compute_output_shape(self, input_shape):
        """ Outputs produced by the layer """
        return [
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))
        ]           
        
data = pd.read_excel("/content/drive/MyDrive/data 13 nov.xlsx")
data.info
data

raw_ques = data['pertanyaan'].tolist()
raw_answ = data['jawaban'].tolist()

def clean_text(txt):
    txt = re.sub("[^a-zA-Z]"," ",str(txt))
    txt = re.sub(r"i'm", "i am", txt)
    txt = re.sub(r"he's", "he is", txt)
    txt = re.sub(r"she's", "she is", txt)
    txt = re.sub(r"that's", "that is", txt)
    txt = re.sub(r"what's", "what is", txt)
    txt = re.sub(r"where's", "where is", txt)
    txt = re.sub(r"\'ll", " will", txt)
    txt = re.sub(r"\'ve", " have", txt)
    txt = re.sub(r"\'re", " are", txt)
    txt = re.sub(r"\'d", " would", txt)
    txt = re.sub(r"won't", "will not", txt)
    txt = re.sub(r"can't", "can not", txt)
    txt = re.sub(r"[^\w\s]", "", txt)
    return txt

ques = []
answ = []

for line in raw_ques:
    ques.append(clean_text(line))
        
for line in raw_answ:
    answ.append(clean_text(line))


#countword
word2count = {}

for line in ques:
    for word in line.split():
        if word not in word2count:
            word2count[word] = 1
        else:
            word2count[word] += 1
for line in answ:
    for word in line.split():
        if word not in word2count:
            word2count[word] = 1
        else:
            word2count[word] += 1

#Vocab
thresh = 3

vocab = {}
word_num = 0
for word, count in word2count.items():
    if count >= thresh:
        vocab[word] = word_num
        word_num += 1


#tokenization
for i in range(len(answ)):
    answ[i] = '<SOS> ' + answ[i] + ' <EOS>'

tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']
x = len(vocab)
for token in tokens:
    vocab[token] = x
    x += 1    

vocab['can'] = vocab['<PAD>']
vocab['<PAD>'] = 0

inv_vocab = {w:v for v, w in vocab.items()}

encoder_inp = []
for line in ques:
    lst = []
    for word in line.split():
        if word not in vocab:
            lst.append(vocab['<OUT>'])
        else:
            lst.append(vocab[word])
        
    encoder_inp.append(lst)

decoder_inp = []
for line in answ:
    lst = []
    for word in line.split():
        if word not in vocab:
            lst.append(vocab['<OUT>'])
        else:
            lst.append(vocab[word])        
    decoder_inp.append(lst)
    
encoder_inp = pad_sequences(encoder_inp, 45, padding='post', truncating='post')
decoder_inp = pad_sequences(decoder_inp, 45, padding='post', truncating='post')


decoder_final_output = []
for i in decoder_inp:
    decoder_final_output.append(i[1:]) 

decoder_final_output = pad_sequences(decoder_final_output, 45, padding='post', truncating='post')

VOCAB_SIZE = len(vocab)
MAX_LEN = 45

print(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])

decoder_final_output = to_categorical(decoder_final_output, len(vocab))

embeddings_index = {}
with open('/content/drive/MyDrive/glove.6B.50d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
   
embedding_dimention = 50
def embedding_matrix_creater(embedding_dimention, word_index):
    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    return embedding_matrix
embedding_matrix = embedding_matrix_creater(50, word_index=vocab) 

embed = Embedding(VOCAB_SIZE+1, 
                  50, 
                  
                  input_length=45,
                  trainable=True)

embed.build((None,))
embed.set_weights([embedding_matrix])

batch_size = 16
epoch = 50
lr = 0.0015
latend_dim = 512

enc_inp = Input(shape=(45, ))
enc_embed = embed(enc_inp)
enc_lstm = Bidirectional(LSTM(latend_dim, return_state=True, dropout=0.5, return_sequences = True))
encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)

state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])
enc_states = [state_h, state_c]

dec_inp = Input(shape=(45, ))
dec_embed = embed(dec_inp)
dec_lstm = LSTM(latend_dim*2, return_state=True, return_sequences=True, dropout=0.5)
decoder_output, _, _,  = dec_lstm(dec_embed, initial_state=enc_states)

attn_layer = AttentionLayer()
attn_op, attn_state = attn_layer([encoder_outputs, decoder_output])
decoder_concat_input = Concatenate(axis=-1)([decoder_output, attn_op])

dec_dense = Dense(VOCAB_SIZE, activation='softmax')
final_output = dec_dense(decoder_concat_input)

model = Model([enc_inp, dec_inp], final_output)
model.summary()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

callback = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
history = model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=epoch, batch_size=batch_size, validation_split=0.1, callbacks=[callback])

plt.plot(history.history['acc']) 
plt.plot(history.history['val_acc']) 
plt.title('model accuracy') 
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left') 
plt.rcParams["figure.figsize"] = (15,8)
plt.show()

model.save('chatbot.h5')
model.save_weights('chatbot_weights.h5')

enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])

decoder_state_input_h_f = tf.keras.layers.Input(shape=( latend_dim*2,))
decoder_state_input_c_f = tf.keras.layers.Input(shape=( latend_dim*2,))
# decoder_state_input_h_b = tf.keras.layers.Input(shape=( latend_dim,))
# decoder_state_input_c_b = tf.keras.layers.Input(shape=( latend_dim,))
decoder_states_inputs = [decoder_state_input_h_f, decoder_state_input_c_f]

decoder_outputs, hf, cf = dec_lstm(dec_embed , initial_state=decoder_states_inputs)

# h = Concatenate()([hf, hb])
# c = Concatenate()([cf, cb])
decoder_states = [hf, cf]

dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],
                                      [decoder_outputs] + decoder_states)
                                
from tensorflow.keras.preprocessing.sequence import pad_sequences
print("******************************************")
print("#       layanan Chatbot COVID-19         #")
print("******************************************")


prepro1 = ""
while prepro1 != 'q':
    
    prepro1 = input("pengguna : ")
    try:
        prepro1 = clean_text(prepro1)
        prepro = [prepro1]
        
        txt = []
        for x in prepro:
            lst = []
            for y in x.split():
                try:
                    lst.append(vocab[y])
                except:
                    lst.append(vocab['<OUT>'])
            txt.append(lst)
        txt = pad_sequences(txt, 45, padding='post')

        enc_op, stat = enc_model.predict( txt )

        empty_target_seq = np.zeros( ( 1 , 1) )
        empty_target_seq[0, 0] = vocab['<SOS>']
        stop_condition = False
        decoded_translation = ''


        while not stop_condition :

            dec_outputs , hf, cf = dec_model.predict([ empty_target_seq ] + stat )

            attn_op, attn_state = attn_layer([enc_op, dec_outputs])
            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])
            decoder_concat_input = dec_dense(decoder_concat_input)

            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )

            sampled_word = inv_vocab[sampled_word_index] + ' '

            if sampled_word != '<EOS> ':
                decoded_translation += sampled_word           

            # if sample_word == '<EOS>' and len(decoded_translation.split()) < 1 :
            #     decoded_translation += sampled_word
            #     stop_condition = False

            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 45:
                stop_condition = True

            empty_target_seq = np.zeros( ( 1 , 1 ) )  
            empty_target_seq[ 0 , 0 ] = sampled_word_index
            stat = [hf, cf] 

        print("chatbot menjawab : ", decoded_translation )
        print("==============================================")

    except:
        print("maaf kurang dimengerti, coba lagi yaa ")                    
